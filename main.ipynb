{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx3DF9ZNXz0v",
        "outputId": "a5976eb6-5cbf-4a2a-bd5f-fcd13906217c"
      },
      "outputs": [],
      "source": [
        "# !git init .\n",
        "# !git remote add origin https://github.com/kashparty/STP-GSR.git\n",
        "# !git pull origin discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OlwY3TLyY9Uh",
        "outputId": "cdad24b0-0e05-4056-ac52-3f7fa221d2a8"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ZM4CkzScXNJk",
        "outputId": "80e2afb2-153a-46b2-cd76-2ceb03a903a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on GPU\n",
            "Training Fold 1/3\n",
            "Model parameters: 317,599\n",
            "STPGSR(\n",
            "  (target_edge_initializer): TargetEdgeInitializer(\n",
            "    (conv1): TransformerConv(160, 67, heads=4)\n",
            "    (bn1): GraphNorm(268)\n",
            "  )\n",
            "  (dual_learner): DualGraphLearner(\n",
            "    (conv1): TransformerConv(3, 1, heads=1)\n",
            "    (bn1): GraphNorm(1)\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (dense_1): Dense()\n",
            "    (relu_1): ReLU(inplace=True)\n",
            "    (dense_2): Dense()\n",
            "    (relu_2): ReLU(inplace=True)\n",
            "    (dense_3): Dense()\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [05:39<00:00,  3.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Generator Loss: 0.19020334449974266, Discriminator Loss: 45.04848217427194\n",
            "Model saved as results/stp_gsr/train/run4/fold_1//model.pth\n",
            "Final Validation Loss (Target): 0.175688236951828\n",
            "Training Fold 2/3\n",
            "Model parameters: 317,599\n",
            "STPGSR(\n",
            "  (target_edge_initializer): TargetEdgeInitializer(\n",
            "    (conv1): TransformerConv(160, 67, heads=4)\n",
            "    (bn1): GraphNorm(268)\n",
            "  )\n",
            "  (dual_learner): DualGraphLearner(\n",
            "    (conv1): TransformerConv(3, 1, heads=1)\n",
            "    (bn1): GraphNorm(1)\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (dense_1): Dense()\n",
            "    (relu_1): ReLU(inplace=True)\n",
            "    (dense_2): Dense()\n",
            "    (relu_2): ReLU(inplace=True)\n",
            "    (dense_3): Dense()\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [05:38<00:00,  3.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Generator Loss: 0.18454023470749725, Discriminator Loss: 45.47690452433921\n",
            "Model saved as results/stp_gsr/train/run4/fold_2//model.pth\n",
            "Final Validation Loss (Target): 0.16140775382518768\n",
            "Training Fold 3/3\n",
            "Model parameters: 317,599\n",
            "STPGSR(\n",
            "  (target_edge_initializer): TargetEdgeInitializer(\n",
            "    (conv1): TransformerConv(160, 67, heads=4)\n",
            "    (bn1): GraphNorm(268)\n",
            "  )\n",
            "  (dual_learner): DualGraphLearner(\n",
            "    (conv1): TransformerConv(3, 1, heads=1)\n",
            "    (bn1): GraphNorm(1)\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (dense_1): Dense()\n",
            "    (relu_1): ReLU(inplace=True)\n",
            "    (dense_2): Dense()\n",
            "    (relu_2): ReLU(inplace=True)\n",
            "    (dense_3): Dense()\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 112/112 [05:40<00:00,  3.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Generator Loss: 0.19373502622225455, Discriminator Loss: 45.10809280297586\n",
            "Model saved as results/stp_gsr/train/run4/fold_3//model.pth\n",
            "Final Validation Loss (Target): 0.17497128248214722\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import hydra\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from hydra import compose, initialize\n",
        "\n",
        "from src.train import train, eval\n",
        "from src.plot_utils import plot_adj_matrices\n",
        "from src.dataset import load_dataset\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def main():\n",
        "    with initialize(version_base=None, config_path=\"configs\"):\n",
        "        config = compose(config_name=\"experiment\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Running on GPU\")\n",
        "    else:\n",
        "        print(\"Running on CPU\")\n",
        "\n",
        "    kf = KFold(n_splits=config.experiment.kfold.n_splits, \n",
        "               shuffle=config.experiment.kfold.shuffle, \n",
        "               random_state=config.experiment.kfold.random_state)\n",
        "\n",
        "    # Initialize folder structure for this run\n",
        "    base_dir = config.experiment.base_dir\n",
        "    model_name = config.model.name\n",
        "    dataset_type = config.dataset.name\n",
        "    run_name = config.experiment.run_name\n",
        "    run_dir = f'{base_dir}/{model_name}/{dataset_type}/{run_name}/'\n",
        "\n",
        "    # Load dataset\n",
        "    source_data, target_data = load_dataset(config)\n",
        "\n",
        "    res_dirs = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(source_data)):\n",
        "        print(f\"Training Fold {fold+1}/3\")\n",
        "\n",
        "        # Initialize results directory\n",
        "        res_dir = f'{run_dir}fold_{fold+1}/'\n",
        "        res_dirs.append(res_dir)\n",
        "        if not os.path.exists(res_dir):\n",
        "            os.makedirs(res_dir)\n",
        "\n",
        "        # Fetch training and val data for this fold\n",
        "        source_data_train = [source_data[i] for i in train_idx]\n",
        "        target_data_train = [target_data[i] for i in train_idx]\n",
        "        source_data_val = [source_data[i] for i in val_idx]\n",
        "        target_data_val = [target_data[i] for i in val_idx]\n",
        "\n",
        "        # Train model for this fold\n",
        "        train_output = train(config, \n",
        "                              source_data_train, \n",
        "                              target_data_train,\n",
        "                              source_data_val,\n",
        "                              target_data_val, \n",
        "                              res_dir)\n",
        "\n",
        "        # Evaluate model for this fold\n",
        "        eval_output, eval_loss = eval(config, \n",
        "                                      train_output['model'], \n",
        "                                      source_data_val, \n",
        "                                      target_data_val, \n",
        "                                      train_output['criterion_L1'])\n",
        "\n",
        "        # Final evaluation loss for this fold\n",
        "        print(f\"Final Validation Loss (Target): {eval_loss}\")\n",
        "\n",
        "        # Save source, taregt, and eval output for this fold\n",
        "        np.save(f'{res_dir}/eval_output.npy', np.array(eval_output))\n",
        "        np.save(f'{res_dir}/source.npy', np.array([s['mat'] for s in source_data_val]))\n",
        "        np.save(f'{res_dir}/target.npy', np.array([t['mat'] for t in target_data_val]))\n",
        "\n",
        "\n",
        "        # Plot predictions for a random sample\n",
        "        idx = 6\n",
        "        source_mat_test = source_data_val[idx]['mat']\n",
        "        target_mat_test = target_data_val[idx]['mat']\n",
        "        eval_output_t = eval_output[idx]\n",
        "\n",
        "        plot_adj_matrices(source_mat_test, \n",
        "                          target_mat_test, \n",
        "                          eval_output_t, \n",
        "                          idx, \n",
        "                          res_dir, \n",
        "                          file_name=f'eval_sample{idx}')\n",
        "\n",
        "    return res_dirs\n",
        "\n",
        "res_dirs = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating each fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtpOsCYZYjFR",
        "outputId": "5525d82b-e032-49cb-cb9b-d1932994cdff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing fold from: results/stp_gsr/train/run4/fold_1/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 45/56 [27:56<06:49, 37.26s/it]\n"
          ]
        },
        {
          "ename": "NetworkXError",
          "evalue": "Graph not connected.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNetworkXError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m gt_ec \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39meigenvector_centrality(gt_graph, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m gt_pc \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mpagerank(gt_graph, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m gt_cf \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_flow_betweenness_centrality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m gt_dc \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mdegree_centrality(gt_graph)\n\u001b[1;32m     52\u001b[0m mae_bc\u001b[38;5;241m.\u001b[39mappend(mean_absolute_error(\u001b[38;5;28mlist\u001b[39m(pred_bc\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(gt_bc\u001b[38;5;241m.\u001b[39mvalues())))\n",
            "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 20:4\u001b[0m, in \u001b[0;36margmap_current_flow_betweenness_centrality_17\u001b[0;34m(G, normalized, weight, dtype, solver)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
            "File \u001b[0;32m/vol/bitbucket/akm20/conda/envs/DGLCW2/lib/python3.11/site-packages/networkx/algorithms/centrality/current_flow_betweenness.py:220\u001b[0m, in \u001b[0;36mcurrent_flow_betweenness_centrality\u001b[0;34m(G, normalized, weight, dtype, solver)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute current-flow betweenness centrality for nodes.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03mCurrent-flow betweenness centrality uses an electrical current\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m   M. E. J. Newman, Social Networks 27, 39-54 (2005).\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mis_connected(G):\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph not connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    221\u001b[0m n \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mnumber_of_nodes()\n\u001b[1;32m    222\u001b[0m ordering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reverse_cuthill_mckee_ordering(G))\n",
            "\u001b[0;31mNetworkXError\u001b[0m: Graph not connected."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from hydra import compose, initialize\n",
        "from src.models.stp_gsr import STPGSR\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from src.matrix_vectorizer import MatrixVectorizer\n",
        "\n",
        "\n",
        "with initialize(version_base=None, config_path=\"configs\"):\n",
        "    config = compose(config_name=\"experiment\")\n",
        "\n",
        "metrics = [\"MAE\", \"PCC\", \"JS\", \"MAE (BC)\", \"MAE (EC)\", \"MAE (PC)\", \"MAE (CF)\", \"MAE (DC)\"]\n",
        "fold_results = {m: [] for m in metrics}\n",
        "\n",
        "# for each fold, compute fold metrics\n",
        "for res_dir in res_dirs:\n",
        "    print(f\"Processing fold from: {res_dir}\")\n",
        "\n",
        "    pred_matrices = np.load(f'{res_dir}/eval_output.npy', allow_pickle=True)\n",
        "    true_matrices = np.load(f'{res_dir}/target.npy', allow_pickle=True)\n",
        "\n",
        "    mae_bc, mae_ec, mae_pc, mae_cf, mae_dc = [], [], [], [], []\n",
        "    pred_1d_list, gt_1d_list = [], []\n",
        "\n",
        "    for i in tqdm(range(len(pred_matrices))):\n",
        "        # turn 0 connections into very small weights for numerical stability\n",
        "        pred_matrices[i] = np.where(\n",
        "            pred_matrices[i] == 0,\n",
        "            1e-10 * np.ones_like(pred_matrices[i]),\n",
        "            pred_matrices[i]\n",
        "        )\n",
        "        true_matrices[i] = np.where(\n",
        "            true_matrices[i] == 0,\n",
        "            1e-10 * np.ones_like(true_matrices[i]),\n",
        "            true_matrices[i]\n",
        "        )\n",
        "        pred_graph = nx.from_numpy_array(pred_matrices[i])\n",
        "        gt_graph = nx.from_numpy_array(true_matrices[i])\n",
        "\n",
        "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
        "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
        "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
        "        pred_cf = nx.current_flow_betweenness_centrality(pred_graph, weight=\"weight\")\n",
        "        pred_dc = nx.degree_centrality(pred_graph)\n",
        "\n",
        "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
        "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
        "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
        "        gt_cf = nx.current_flow_betweenness_centrality(gt_graph, weight=\"weight\")\n",
        "        gt_dc = nx.degree_centrality(gt_graph)\n",
        "\n",
        "        mae_bc.append(mean_absolute_error(list(pred_bc.values()), list(gt_bc.values())))\n",
        "        mae_ec.append(mean_absolute_error(list(pred_ec.values()), list(gt_ec.values())))\n",
        "        mae_pc.append(mean_absolute_error(list(pred_pc.values()), list(gt_pc.values())))\n",
        "        mae_cf.append(mean_absolute_error(list(pred_cf.values()), list(gt_cf.values())))\n",
        "        mae_dc.append(mean_absolute_error(list(pred_dc.values()), list(gt_dc.values())))\n",
        "\n",
        "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
        "        gt_1d_list.append(MatrixVectorizer.vectorize(true_matrices[i]))\n",
        "\n",
        "    # Aggregate per-fold results\n",
        "    pred_1d = np.concatenate(pred_1d_list)\n",
        "    gt_1d = np.concatenate(gt_1d_list)\n",
        "\n",
        "    fold_results[\"MAE\"].append(mean_absolute_error(pred_1d, gt_1d))\n",
        "    fold_results[\"PCC\"].append(pearsonr(pred_1d, gt_1d)[0])\n",
        "    fold_results[\"JS\"].append(jensenshannon(pred_1d, gt_1d))\n",
        "    fold_results[\"MAE (BC)\"].append(np.mean(mae_bc))\n",
        "    fold_results[\"MAE (EC)\"].append(np.mean(mae_ec))\n",
        "    fold_results[\"MAE (PC)\"].append(np.mean(mae_pc))\n",
        "    fold_results[\"MAE (CF)\"].append(np.mean(mae_cf))\n",
        "    fold_results[\"MAE (DC)\"].append(np.mean(mae_dc))\n",
        "\n",
        "# average over all folds, with error bars\n",
        "means = [np.mean(fold_results[m]) for m in metrics]\n",
        "stds = [np.std(fold_results[m]) for m in metrics]\n",
        "\n",
        "\n",
        "# polts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
        "axes = axes.flatten()\n",
        "colors = ['red', 'green', 'blue', 'purple', 'orange', 'cyan', 'yellow', 'pink']\n",
        "for i, res_dir in enumerate(res_dirs):\n",
        "    axes[i].bar(metrics, [fold_results[m][i] for m in metrics], color=colors)\n",
        "    axes[i].set_title(f'Fold {i+1}')z\n",
        "    axes[i].set_ylim(0, 1)\n",
        "\n",
        "axes[3].bar(metrics, means, yerr=stds, capsize=5, color=colors)\n",
        "axes[3].set_title('Avg. Across Folds')\n",
        "axes[3].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training final model and running predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWH3waODaOcz"
      },
      "outputs": [],
      "source": [
        "from src.dataset import create_pyg_graph\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "source_vectorized = np.genfromtxt(\"lr_test.csv\", delimiter=\",\", skip_header=1)\n",
        "source_mat_all = [MatrixVectorizer.anti_vectorize(A, 160) for A in source_vectorized]\n",
        "\n",
        "source_mat_all = [torch.tensor(x, dtype=torch.float) for x in source_mat_all]\n",
        "pyg_partial = partial(create_pyg_graph, node_feature_init=\"adj\", node_feat_dim=160)\n",
        "\n",
        "source_pyg_all = [pyg_partial(x, 160) for x in source_mat_all]\n",
        "source_data = [{'pyg': source_pyg, 'mat': source_mat} for source_pyg, source_mat in zip(source_pyg_all, source_mat_all)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZO80AYpazB9",
        "outputId": "d2b166a0-9c98-40d7-c80e-26745d9a7421"
      },
      "outputs": [],
      "source": [
        "from src.dual_graph_utils import revert_dual\n",
        "\n",
        "model.eval()\n",
        "eval_output = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for source in tqdm(source_data):\n",
        "        source_g = source['pyg']\n",
        "\n",
        "        model_pred, model_target = model(source_g, None)\n",
        "        pred_m = revert_dual(model_pred, 268)    # (n_t, n_t)\n",
        "        pred_m = pred_m.cpu().numpy()\n",
        "        eval_output.append(pred_m)\n",
        "\n",
        "eval_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Submission Generation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "O_tiSfC2cFqC",
        "outputId": "6915624d-d3a4-4c23-d6b0-38c2e718bdbd"
      },
      "outputs": [],
      "source": [
        "from src.matrix_vectorizer import MatrixVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "test_array = np.concatenate([MatrixVectorizer.vectorize(eo) for eo in eval_output])\n",
        "\n",
        "output_df = pd.DataFrame({\"Predicted\": test_array.flatten()})\n",
        "output_df.index = np.arange(1, len(output_df) + 1)\n",
        "output_df.to_csv(\"submission.csv\", index_label=\"ID\")\n",
        "output_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DGLCW2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
